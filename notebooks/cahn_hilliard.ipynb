{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cahn-Hilliard Example\n",
    "\n",
    "This example demonstrates how to use PyMKS to solve the Cahn-Hilliard equation. The first section provides some background information about the Cahn-Hilliard equation as well as details about calibrating and validating the MKS model. The example demonstrates how to generate sample data, calibrate the influence coefficients and then pick an appropriate number of local states when state space is continuous. The MKS model and a spectral solution of the Cahn-Hilliard equation are compared on a larger test microstructure over multiple time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cahn-Hilliard Equation\n",
    "\n",
    "The Cahn-Hilliard equation is used to simulate microstructure evolution during spinodial decomposition and has the following form,\n",
    "\n",
    "$$ \\dot{\\phi} = \\nabla^2 \\left( \\phi^3 - \\phi \\right) - \\gamma \\nabla^4 \\phi $$\n",
    "\n",
    "where $\\phi$ is a conserved ordered parameter and $\\sqrt{\\gamma}$ represents the width of the interface. In this example, the Cahn-Hilliard equation is solved using a semi-implicit spectral scheme with periodic boundary conditions, see  [Chang and Rutenberg](http://dx.doi.org/10.1103/PhysRevE.72.055701) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modeling with MKS\n",
    "\n",
    "In this example the MKS equation will be used to predict microstructure at the next time step using \n",
    "\n",
    "$$p[s, 1] = \\sum_{r=0}^{S-1} \\alpha[l, r, 1] \\sum_{l=0}^{L-1} m[l, s - r, 0] + ...$$\n",
    "\n",
    "where $p[s, n + 1]$ is the concentration field at location $s$ and at time $n + 1$, $r$ is the convolution dummy variable and $l$ indicates the local states varable. $\\alpha[l, r, n]$ are the influence coefficients and $m[l, r, 0]$ the microstructure function given to the model. $S$ is the total discretized volume and $L$ is the total number of local states `n_states` choosen to use.\n",
    "\n",
    "The model will march forward in time by recussively replacing discretizing $p[s, n]$ and substituing it back for $m[l, s - r, n]$.\n",
    "\n",
    "###Calibration Datasets\n",
    "\n",
    "Unlike the elastostatic examples, the microstructure (concentration field) for this simulation doesn't have discrete phases. The microstructure is a continuous field that can have a range of values which can change over time, therefore the first order influence coefficients cannot be calibrated with delta microstructures. Instead a large number of simulations with random initial conditions are used to calibrate the first order influence coefficients using linear regression.\n",
    "\n",
    "The function `make_cahn_hilliard` from `pymks.datasets` provides an interface to generate calibration datasets for the influence coefficients. To use `make_cahn_hilliard`, we need to set the number of samples we want to use to calibrate the influence coefficients using `n_samples`, the size of the simulation domain using `size` and the time step using `dt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymks\n",
    "from pymks.datasets import make_cahn_hilliard\n",
    "\n",
    "n = 41\n",
    "n_samples = 400\n",
    "dt = 1e-2\n",
    "np.random.seed(99)\n",
    "= make_cahn_hilliard(n_samples=n_samples, size=(n, n), dt=dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `make_cahnHilliard` generates `n_samples` number of random microstructures, `X`, and the associated updated microstructures, `y`, after one time step `y`. The following cell plots one of these microstructures along with its update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_concentrations\n",
    "\n",
    "draw_concentrations((, ), labels=('Input', 'Out'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate Influence Coefficients\n",
    "\n",
    "As mentioned above, the microstructures (concentration fields) does not have discrete phases. This leaves the number of local states in local state space as a free hyper parameter. In previous work it has been shown that as you increase the number of local states, the accuracy of MKS model increases (see [Fast et al.](http://dx.doi.org/10.1016/j.actamat.2010.10.008)), but as the number of local states increases, the difference in accuracy decreases. Some work needs to be done in order to find the practical number of local states that we will use. \n",
    "\n",
    "#### Optimizing the Number of Local States\n",
    "\n",
    "Let's split the calibrate dataset into testing and training datasets. The function `train_test_split` for the machine learning python module [`sklearn`](http://scikit-learn.org/stable/) provides a convenient interface to do this. 80% of the dataset will be used for training and the remaining 20% will be used for testing by setting `test_size` equal to 0.2. The state of the random number generator used to make the split can be set using `random_state`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "split_shape = (X.shape[0],) + (np.product(X.shape[1:]),)\n",
    "= train_test_split(X.reshape(split_shape), y.reshape(split_shape),\n",
    "                                                    test_size=0.5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to calibrate the influence coefficients while varying the number of local states from 2 up to 20. Each of these models will then predict the evolution of the concentration fields. Mean square error will be used to compared the results with the testing dataset to evaluate how the MKS model's performance changes as we change the number of local states.  \n",
    "\n",
    "First we need to import the class `MKSLocalizationModel` from `pymks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks import MKSLocalizationModel\n",
    "from pymks.bases import PrimitiveBasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will calibrate the influence coefficients while varying the number of local states and compute the mean squared error. The following demonstrates how to use Scikit-learn's `GridSearchCV` to optimize `n_states` as a hyperparameter. Of course, the best fit is always with a larger value of `n_states`. Increasing this parameter does not overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters_to_tune = {'n_states': np.arange(2, 11)}\n",
    "prim_basis = PrimitiveBasis(2, [-1, 1])\n",
    "model = MKSLocalizationModel(prim_basis)\n",
    "gs = GridSearchCV(model, parameters_to_tune, cv=5, fit_params={'size': (n, n)})\n",
    "gs.fit(, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gs.best_estimator_)\n",
    "print(gs.score(, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_gridscores\n",
    "\n",
    "draw_gridscores(gs.grid_scores_, 'n_states',\n",
    "                score_label='R-squared', param_label='L - Number of states')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the accuracy of the MKS model monotonically increases as we increase n_states, but accuracy doesn't improve significantly as n_states gets larger than signal digits. \n",
    "\n",
    "In order to save on computation costs let's set calibrate the influence coefficients with `n_states` equal to 6, but realize that if we need slightly more accuracy the value can be increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MKSLocalizationModel(basis=PrimitiveBasis(6, [-1, 1]))\n",
    "model.fit(,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 4 influence coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_coeff\n",
    "\n",
    "draw_coeff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Predict Microstructure Evolution\n",
    "\n",
    "With the calibrated influence coefficients, we are ready to predict the evolution of a concentration field. In order to do this, we need to have the Cahn-Hilliard simulation and the MKS model start with the same initial concentration `phi0` and evolve in time. In order to do the Cahn-Hilliard simulation we need an instance of the class `CahnHilliardSimulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.datasets.cahn_hilliard_simulation import CahnHilliardSimulation\n",
    "np.random.seed(191)\n",
    "\n",
    "phi0 = np.random.normal(0, 1e-9, (1, n, n))\n",
    "ch_sim = CahnHilliardSimulation(dt=dt)\n",
    "phi_sim = phi0.copy()\n",
    "phi_pred = phi0.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to move forward in time, we need to feed the concentration back into the Cahn-Hilliard simulation and the MKS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_steps = 10\n",
    "\n",
    "for ii in range(time_steps):\n",
    "    ch_sim.run(phi_sim)\n",
    "    phi_sim = ch_sim.response\n",
    "    phi_pred = model.predict(phi_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the concentration fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_concentrations_compare\n",
    "\n",
    "draw_concentrations_compare((phi_sim[0], phi_pred[0]), \n",
    "                            labels=('Simulation', 'MKS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MKS model was able to capture the microstructure evolution with 6 local states. \n",
    "\n",
    "##Resizing the Coefficients to use on Larger Systems \n",
    "\n",
    "Now let's try and predict a larger simulation by resizing the coefficients and provide a larger initial concentratio field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 3 * n\n",
    "\n",
    "phi0 = np.random.normal(0, 1e-9, (1, m, m))\n",
    "phi_sim = phi0.copy()\n",
    "phi_pred = phi0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we are going to march forward in time by feeding the concentration fields back into the Cahn-Hilliard simulation and the MKS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ii in range(1000):\n",
    "    ch_sim.run(phi_sim)\n",
    "    phi_sim = ch_sim.response\n",
    "    phi_pred = model.predict(phi_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_concentrations_compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MKS model with resized influence coefficients was able to reasonably predict the structure evolution for a larger concentration field. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
